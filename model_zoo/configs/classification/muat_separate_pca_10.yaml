# Pattern Classifier Configuration for muat-separate-pca-10 Dataset
# Matches dataset generated from: dataset_generation/exp_1/small_simple_separate_pca_10.yaml

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
dataset:
  hf_dataset: "maximuspowers/muat-mean-std-pca-10-fourier-5"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  random_seed: 42

  # Input mode: "signature", "weights", or "both"
  input_mode: "signature"

  # Max dimensions matching the dataset generation config
  max_dimensions:
    max_layers: 13         # Actual dataset has layers 0-12 (13 total)
    max_neurons_per_layer: 8   # Dataset has 5-8 neurons per layer
    max_sequence_length: 5     # Dataset uses sequence_length: 5

  # Neuron profiling configuration (must match dataset generation)
  neuron_profile:
    methods:
      mean: {}
      std: {}
      pca: {components: 10}
      fourier: {n_frequencies: 5}

  # Pattern list (from dataset generation config)
  patterns:
    - "palindrome"
    - "sorted_ascending"
    - "sorted_descending"
    - "alternating"
    - "contains_abc"
    - "starts_with"
    - "ends_with"
    - "no_repeats"
    - "has_majority"
    - "increasing_pairs"
    - "decreasing_pairs"
    - "vowel_consonant"
    - "first_last_match"
    - "mountain_pattern"

# ============================================================================
# MODEL ARCHITECTURE
# ============================================================================
# Note: hidden_dims controls both number of layers and their sizes
# Each element in the list represents one layer's output size
# If hidden_dims is not specified, it will be auto-computed from input dimensions
# Examples:
#   hidden_dims: [256, 128]           → 2 layers (256 units, then 128 units)
#   hidden_dims: [512, 256, 128, 64]  → 4 layers (512 → 256 → 128 → 64)
model:
  # Use batch normalization to help with deep networks and prevent local minima
  use_batch_norm: true

  signature_encoder:
    dropout: 0.2  # Increased slightly - batch norm helps with regularization
    activation: "relu"
    hidden_dims: [512, 256, 256, 128]  # Simplified: 3 layers instead of 9 (less prone to vanishing gradients)

  weight_encoder:
    dropout: 0.2
    activation: "relu"
    # hidden_dims: [512, 256]  # Optional: list of layer sizes (length = number of layers)

  fusion:
    dropout: 0.2
    activation: "relu"
    # hidden_dims: [128, 64]   # Optional: list of layer sizes (length = number of layers)

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  optimizer: "adam"
  learning_rate: 0.001  # Increased from 0.0001 - higher LR helps escape local minima
  weight_decay: 0.0001
  epochs: 1000
  batch_size: 16

  # Early stopping
  early_stopping:
    enabled: true
    patience: 50  # Give model time to converge
    monitor: "val_loss"  # Monitor F1 score for best performance
    mode: "min"

  # Loss function
  loss: "bce_with_logits"
  pos_weight: null  # Auto-compute from data

  # Learning rate scheduler - less aggressive decay
  lr_scheduler:
    enabled: true
    type: "reduce_on_plateau"
    patience: 20  # Increased from 5 - wait longer before reducing LR
    factor: 0.5
    min_lr: 0.00001  # Increased from 0.000001 - don't go too low

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
evaluation:
  decision_threshold: 0.5

  metrics:
    - "accuracy_exact_match"
    - "accuracy_hamming"
    - "precision_macro"
    - "recall_macro"
    - "f1_macro"
    - "f1_micro"

  per_pattern_metrics: true

# ============================================================================
# LOGGING & CHECKPOINTING
# ============================================================================
# Note: Output directories (logs, checkpoints, cache) are automatically managed by the CLI
logging:
  # TensorBoard
  tensorboard:
    enabled: true
    log_interval: 10

  # Checkpointing
  checkpoint:
    enabled: true
    save_best_only: true
    monitor: "val_f1_macro"  # Save model with best F1 score
    mode: "max"

  verbose: true

# ============================================================================
# HUGGINGFACE HUB INTEGRATION
# ============================================================================
hub:
  enabled: true
  repo_id: "maximuspowers/test"
  token: ""
  private: false
  push_model: true
  push_logs: true
  push_metrics: true
  push_frequency: "epoch"

# ============================================================================
# DEVICE & DATA LOADING
# ============================================================================
device:
  type: "auto"  # auto, cuda, mps, cpu

dataloader:
  num_workers: 0
  pin_memory: true

# ============================================================================
# RUN DIRECTORY MANAGEMENT
# ============================================================================
run_log_cleanup: true # deletes the run dir after completing the run if true
