{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Installs and Set Up\n\n# First, uninstall any existing torch installations to avoid conflicts\n!pip uninstall -y torch torchvision torchaudio\n\n# Install compatible PyTorch and torchvision versions for CUDA 11.8\n!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n\n# Install dependencies for HuggingFace SFT training\n!pip install -q transformers>=4.36.0\n!pip install -q accelerate>=0.21.0\n!pip install -q peft>=0.7.0\n!pip install -q trl>=0.7.0\n!pip install -q bitsandbytes>=0.41.0\n!pip install -q datasets>=2.17.1\n!pip install -q wandb>=0.16.3\n\n# Restart runtime to ensure clean import\nimport os\nprint(\"‚ö†Ô∏è  Please restart the runtime after installation completes to ensure clean imports\")\nprint(\"   Go to Runtime -> Restart Runtime, then continue with the next cells\")\n\n# Login to Hugging Face (run this after restart)\ntry:\n    from huggingface_hub import login\n    print(\"Please log in to Hugging Face to upload your fine-tuned model:\")\n    token = \"\" #@param {type:\"string\"}\n    if not token:\n        token = input(\"Enter your HuggingFace token: \")\n    login(token=token)\n    print(\"‚úÖ Successfully logged in to HuggingFace\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  HuggingFace login will be handled in next cell: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Verify Installation (Run After Restart)\n\n# Verify PyTorch installation\ntry:\n    import torch\n    import torchvision\n    print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n    print(f\"‚úÖ Torchvision version: {torchvision.__version__}\")\n    print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n        print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\nexcept Exception as e:\n    print(f\"‚ùå PyTorch installation issue: {e}\")\n\n# Verify other dependencies\ntry:\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    from trl import SFTTrainer\n    from datasets import load_dataset\n    print(\"‚úÖ Transformers imported successfully\")\n    print(\"‚úÖ TRL SFTTrainer imported successfully\")\n    print(\"‚úÖ Datasets imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå Dependency import issue: {e}\")\n\n# Login to HuggingFace if not done already\ntry:\n    from huggingface_hub import login\n    token = \"\" #@param {type:\"string\"}\n    if not token:\n        token = input(\"Enter your HuggingFace token: \")\n    login(token=token)\n    print(\"‚úÖ Successfully logged in to HuggingFace\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  HuggingFace login issue: {e}\")\n\nprint(\"\\nüéØ If all checks pass, you can proceed to the Config cell\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Config\n",
    "\n",
    "MODEL_NAME = \"maximuspowers/starcoder2-7b-interpreter\" #@param\n",
    "DATASET_NAME = \"maximuspowers/llm-interpretability-v1\" #@param\n",
    "BASE_MODEL = \"bigcode/starcoder2-7b\" #@param\n",
    "\n",
    "# Training parameters optimized for SFTTrainer\n",
    "MAX_SEQ_LENGTH = 4096 #@param\n",
    "NUM_TRAIN_EPOCHS = 1 #@param\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1 #@param {type:\"integer\"}\n",
    "GRADIENT_ACCUMULATION_STEPS = 8 #@param {type:\"integer\"}\n",
    "LEARNING_RATE = 5e-7 #@param\n",
    "WARMUP_STEPS = 100 #@param\n",
    "LOGGING_STEPS = 10 #@param\n",
    "SAVE_STEPS = 100 #@param\n",
    "PUSH_TO_HUB = True #@param {type:\"boolean\"}\n",
    "USE_QUANTIZATION = True #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "source": "#@title Training with LoRA and SFTTrainer\n\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\nimport os\n\nprint(\"üöÄ Starting LoRA fine-tuning with SFTTrainer...\")\n\n# Load dataset\nprint(f\"üìö Loading dataset: {DATASET_NAME}\")\ntry:\n    dataset = load_dataset(DATASET_NAME)\n    data = dataset[\"train\"]\n    print(f\"‚úÖ Dataset loaded: {len(data)} training examples\")\nexcept Exception as e:\n    print(f\"‚ùå Failed to load dataset: {e}\")\n    print(\"Make sure the dataset exists and is accessible\")\n    raise\n\n# Setup quantization config for memory efficiency\nprint(\"‚öôÔ∏è  Setting up 4-bit quantization...\")\nif USE_QUANTIZATION:\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\nelse:\n    bnb_config = None\n\n# Load base model\nprint(f\"üß† Loading base model: {BASE_MODEL}\")\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        BASE_MODEL,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16,\n        use_flash_attention_2=False  # Fallback for compatibility\n    )\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        \n    print(\"‚úÖ Model and tokenizer loaded successfully\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Failed to load model: {e}\")\n    raise\n\n# Prepare model for LoRA training\nif USE_QUANTIZATION:\n    print(\"üîß Preparing model for k-bit training...\")\n    model = prepare_model_for_kbit_training(model)\n\n# Setup LoRA configuration\nprint(\"üéõÔ∏è  Setting up LoRA configuration...\")\nlora_config = LoraConfig(\n    r=16,                    # Rank\n    lora_alpha=32,          # Alpha scaling parameter\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.1,       # Dropout probability\n    bias=\"none\",            # Bias type\n    task_type=\"CAUSAL_LM\"   # Task type\n)\n\n# Create training arguments\nprint(\"üìã Setting up training arguments...\")\ntraining_args = transformers.TrainingArguments(\n    output_dir=\"./starcoder2_7b_sft_output\",\n    num_train_epochs=NUM_TRAIN_EPOCHS,\n    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    warmup_steps=WARMUP_STEPS,\n    learning_rate=LEARNING_RATE,\n    fp16=False,\n    bf16=True,\n    logging_steps=LOGGING_STEPS,\n    optim=\"adamw_8bit\" if USE_QUANTIZATION else \"adamw_torch\",\n    evaluation_strategy=\"no\",  # No validation data\n    save_strategy=\"steps\",\n    save_steps=SAVE_STEPS,\n    group_by_length=True,\n    report_to=None,  # Disable wandb logging\n    run_name=f\"starcoder2-interpreter-{NUM_TRAIN_EPOCHS}ep\",\n    max_grad_norm=1.0,\n    remove_unused_columns=False,\n    dataloader_pin_memory=False,\n    push_to_hub=PUSH_TO_HUB,\n    hub_model_id=MODEL_NAME if PUSH_TO_HUB else None,\n    hub_strategy=\"checkpoint\",\n)\n\n# Create SFTTrainer\nprint(\"üèãÔ∏è Setting up SFTTrainer...\")\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=data,\n    args=training_args,\n    peft_config=lora_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    max_seq_length=MAX_SEQ_LENGTH,\n    packing=False,  # Don't pack multiple examples together\n)\n\n# Start training\nprint(\"üî• Starting training...\")\nprint(f\"üìä Training configuration:\")\nprint(f\"   Model: {BASE_MODEL}\")\nprint(f\"   Dataset: {DATASET_NAME} ({len(data)} examples)\")\nprint(f\"   Epochs: {NUM_TRAIN_EPOCHS}\")\nprint(f\"   Batch size: {PER_DEVICE_TRAIN_BATCH_SIZE}\")\nprint(f\"   Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"   Learning rate: {LEARNING_RATE}\")\nprint(f\"   Max sequence length: {MAX_SEQ_LENGTH}\")\nprint(f\"   LoRA rank: {lora_config.r}\")\nprint(f\"   Using quantization: {USE_QUANTIZATION}\")\nprint(f\"   Push to Hub: {PUSH_TO_HUB}\")\n\ntry:\n    # Train the model\n    trainer.train()\n    \n    # Save the final model\n    print(\"üíæ Saving final model...\")\n    final_output_dir = \"./starcoder2_7b_sft_output/final_checkpoint\"\n    trainer.save_model(final_output_dir)\n    \n    # Save tokenizer as well\n    tokenizer.save_pretrained(final_output_dir)\n    \n    print(f\"‚úÖ Model saved to: {final_output_dir}\")\n    \n    # Push to Hub if requested\n    if PUSH_TO_HUB:\n        print(f\"üì§ Pushing model to HuggingFace Hub: {MODEL_NAME}\")\n        try:\n            # Push the adapter to hub\n            trainer.model.push_to_hub(MODEL_NAME)\n            tokenizer.push_to_hub(MODEL_NAME)\n            print(f\"‚úÖ Model successfully pushed to: https://huggingface.co/{MODEL_NAME}\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Failed to push to hub: {e}\")\n            print(\"   Model is still saved locally and can be used for evaluation\")\n    \n    # Training completed successfully\n    print(\"üéâ Training completed successfully!\")\n    print(f\"üìÅ Local model path: {final_output_dir}\")\n    if PUSH_TO_HUB:\n        print(f\"ü§ó HuggingFace model: {MODEL_NAME}\")\n    print(\"üéØ Ready for evaluation!\")\n    \n    # Cleanup GPU memory\n    del trainer, model\n    torch.cuda.empty_cache()\n    \nexcept Exception as e:\n    print(f\"‚ùå Training failed: {e}\")\n    import traceback\n    traceback.print_exc()\n    \n    # Cleanup on failure\n    if 'trainer' in locals():\n        del trainer\n    if 'model' in locals():\n        del model\n    torch.cuda.empty_cache()\n    raise",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Inference\n",
    "\n",
    "print(\"üß™ Testing the fine-tuned model...\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Choose model path based on whether we pushed to hub\n",
    "model_path = MODEL_NAME if PUSH_TO_HUB else \"./starcoder2_7b_sft_final\"\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model\n",
    "    print(f\"üî§ Loading tokenizer from: {model_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"üß† Loading model from: {model_path}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Create a test prompt in the training format\n",
    "    test_prompt = \"\"\"# Neural Network Weight Modification Task\n",
    "\n",
    "You are an expert neural network interpreter. Your task is to analyze the given model weights and baseline features, then generate improved weights that will make the model correctly classify the specified pattern\n",
    "\n",
    "## Target Pattern\n",
    "Pattern Name: palindrome\n",
    "Description: Sequence reads same forwards and backwards\n",
    "\n",
    "The model should classify sequences matching this pattern as POSITIVE (label=1).\n",
    "\n",
    "## Model Architecture\n",
    "Input Size: 49 (7 tokens √ó 7 positions, one-hot encoded)\n",
    "Hidden Layers: 6\n",
    "Neurons per Layer: 30\n",
    "Activation Function: relu\n",
    "Dropout Rate: 0.1\n",
    "\n",
    "## Current Model Weights\n",
    "The model weights that need to be improved:\n",
    "\n",
    "{'network.0.weight': [[0.1, 0.2, -0.1]], 'network.0.bias': [0.0]}\n",
    "\n",
    "## Individual Neuron Activations\n",
    "Baseline activations for each neuron (statistics extracted by processing a standard baseline dataset through the model):\n",
    "\n",
    "Layer 0: mean=[0.1, 0.2, 0.3], std=[0.05, 0.08, 0.12]\n",
    "\n",
    "## Generate Improved Model Weights\n",
    "\n",
    "Here are the improved model weights that will correctly classify the target pattern:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    print(\"üî• Generating test response...\")\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode and print result\n",
    "    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(\"üìù Test generation:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Input: [Neural network weight modification prompt for palindrome pattern]\")\n",
    "    print(\"\\nGenerated completion:\")\n",
    "    print(generated_text[:500] + \"...\" if len(generated_text) > 500 else generated_text)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Check if output looks like model weights\n",
    "    if '{' in generated_text and 'network.' in generated_text:\n",
    "        print(\"‚úÖ Model appears to be generating weight-like structures\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Model output doesn't clearly contain weight structures\")\n",
    "        \n",
    "    print(\"\\nüéØ Inference test complete!\")\n",
    "    print(\"   The model is responding to neural network interpretation prompts\")\n",
    "    print(\"   Ready for full evaluation!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Inference test failed: {e}\")\n",
    "    print(\"Make sure training completed successfully\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Evaluation\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Setup logging for the notebook\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# First, create benchmark dataset if it doesn't exist\n",
    "from benchmark_generator import create_benchmark_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "benchmark_path = Path(\"benchmark_dataset.json\")\n",
    "if not benchmark_path.exists():\n",
    "    print(\"üìã Creating benchmark dataset...\")\n",
    "    create_benchmark_dataset(samples_per_pattern=35)\n",
    "    print(\"‚úÖ Benchmark dataset created!\")\n",
    "else:\n",
    "    print(\"üìã Benchmark dataset already exists\")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"üß™ Starting interpreter evaluation...\")\n",
    "print(\"‚ö†Ô∏è  Note: This will take a significant amount of time (several hours)\")\n",
    "print(\"üî• Using trained model:\", MODEL_NAME)\n",
    "\n",
    "try:\n",
    "    from evaluation import InterpreterEvaluator\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = InterpreterEvaluator(\n",
    "        interpreter_model=MODEL_NAME,\n",
    "        benchmark_path=\"benchmark_dataset.json\",\n",
    "        baseline_path=\"../training_data/baseline_dataset.json\",\n",
    "        device=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ Will evaluate {len(evaluator.all_patterns)} patterns with 10 trials each (150 total tasks)\")\n",
    "    print(f\"üìä Benchmark dataset: {evaluator.benchmark_dataset['num_examples']} examples\")\n",
    "    \n",
    "    # Run evaluation (this will take a long time!)\n",
    "    results = evaluator.run_full_evaluation(\n",
    "        save_results=True,\n",
    "        results_filename=\"evaluation_results.json\"\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"üìà Overall Success Rate: {results['overall_success_rate']:.1%}\")\n",
    "    print(f\"üìä Average Pattern Improvement: {results['overall_avg_improvement']:.2f} detections\")\n",
    "    print(f\"‚úÖ Completed Tasks: {results['completed_tasks']}/{results['total_tasks']}\")\n",
    "    \n",
    "    print(f\"\\nüìã Pattern-by-Pattern Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Pattern':<20} | {'Success Rate':<12} | {'Avg Œî':<8} | {'Tasks':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for pattern, stats in results['pattern_stats'].items():\n",
    "        success_rate = f\"{stats['success_rate']:.1%}\"\n",
    "        avg_improvement = f\"{stats['avg_improvement']:+.1f}\"\n",
    "        tasks = f\"{stats['completed_tasks']}/{stats['total_tasks']}\"\n",
    "        \n",
    "        print(f\"{pattern:<20} | {success_rate:<12} | {avg_improvement:<8} | {tasks:<10}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(\"üíæ Detailed results saved to: evaluation_results.json\")\n",
    "    \n",
    "    # Load and display top successes\n",
    "    try:\n",
    "        with open(\"evaluation_results.json\", 'r') as f:\n",
    "            detailed = json.load(f)\n",
    "        \n",
    "        successful_results = [r for r in detailed['detailed_results'] if r.get('success', False)]\n",
    "        successful_results.sort(key=lambda x: x['target_improvement'], reverse=True)\n",
    "        \n",
    "        if successful_results:\n",
    "            print(f\"\\nüèÜ Top 5 Most Successful Pattern Additions:\")\n",
    "            print(\"-\" * 60)\n",
    "            for i, result in enumerate(successful_results[:5]):\n",
    "                target = result['target_pattern']\n",
    "                improvement = result['target_improvement']\n",
    "                before = result['before_target_count']\n",
    "                after = result['after_target_count']\n",
    "                print(f\"{i+1}. {target}: {before} ‚Üí {after} (+{improvement} detections)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not display detailed results: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Evaluation Complete!\")\n",
    "    print(f\"   ‚Ä¢ Success rate: {results['overall_success_rate']:.1%} of pattern additions succeeded\")\n",
    "    print(f\"   ‚Ä¢ Average improvement: {results['overall_avg_improvement']:.1f} additional detections per successful task\")\n",
    "    print(f\"   ‚Ä¢ Results saved to evaluation_results.json for further analysis\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed: {e}\")\n",
    "    print(\"Make sure the model trained successfully and all dependencies are available\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Evaluation (Colab - with Git Clone)\n\nimport sys\nimport json\nimport logging\nfrom IPython.display import display, HTML\nimport os\n\n# Setup logging for the notebook\nlogging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n\n# Check if we're in Colab and need to clone the repo\nif not os.path.exists('/content/llm-interpretability'):\n    # Clone the project repository to access evaluation code\n    print(\"üì• Cloning project repository...\")\n    !git clone https://github.com/maximus-powers/llm-interpretability.git /content/llm-interpretability\n    print(\"‚úÖ Repository cloned successfully\")\nelse:\n    print(\"üìÅ Repository already exists, skipping clone\")\n\n# Add the project paths to Python path\nsys.path.insert(0, '/content/llm-interpretability/interpreter')\nsys.path.insert(0, '/content/llm-interpretability/training_data')\n\n# Change to the interpreter directory\nos.chdir('/content/llm-interpretability/interpreter')\n\n# Import the modules after setting up paths\ntry:\n    from benchmark_generator import create_benchmark_dataset\n    from evaluation import InterpreterEvaluator\n    print(\"‚úÖ Successfully imported evaluation modules\")\nexcept ImportError as e:\n    print(f\"‚ùå Failed to import modules: {e}\")\n    print(\"üîÑ Retrying after path setup...\")\n    # Add fallback paths\n    sys.path.append('/content/llm-interpretability/interpreter')\n    sys.path.append('/content/llm-interpretability/training_data')\n    from benchmark_generator import create_benchmark_dataset\n    from evaluation import InterpreterEvaluator\n\nfrom pathlib import Path\n\n# First, create benchmark dataset if it doesn't exist\nbenchmark_path = Path(\"benchmark_dataset.json\")\nif not benchmark_path.exists():\n    print(\"üìã Creating benchmark dataset...\")\n    create_benchmark_dataset(samples_per_pattern=35)\n    print(\"‚úÖ Benchmark dataset created!\")\nelse:\n    print(\"üìã Benchmark dataset already exists\")\n\n# Determine model path (prioritize local paths, then HuggingFace Hub)\nprint(\"üîç Looking for trained model...\")\n\n# Check local paths first\nlocal_paths = [\n    \"./starcoder2_7b_sft_output/final_checkpoint/\",\n    \"/content/starcoder2_7b_sft_output/final_checkpoint/\",\n    \"../starcoder2_7b_sft_output/final_checkpoint/\"\n]\n\nmodel_to_use = None\nfor path in local_paths:\n    if os.path.exists(path):\n        model_to_use = path\n        print(f\"üìÅ Found local model: {model_to_use}\")\n        break\n\n# If no local model found, try HuggingFace Hub\nif model_to_use is None:\n    if 'MODEL_NAME' in globals() and MODEL_NAME:\n        model_to_use = MODEL_NAME\n        print(f\"ü§ó Will try HuggingFace model: {model_to_use}\")\n        print(\"‚ö†Ô∏è  Note: Make sure the model exists on HuggingFace Hub with LoRA adapter files\")\n    else:\n        # Last resort - use base model\n        model_to_use = \"bigcode/starcoder2-7b\"\n        print(f\"üîÑ No trained model found, using base model: {model_to_use}\")\n        print(\"‚ö†Ô∏è  Warning: Using base model - evaluation may not show fine-tuning effects\")\n\n# Run evaluation\nprint(\"üß™ Starting interpreter evaluation...\")\nprint(\"‚ö†Ô∏è  Note: This will take a significant amount of time (several hours)\")\nprint(f\"üî• Using model: {model_to_use}\")\n\ntry:\n    # Initialize evaluator (using paths relative to cloned repo)\n    evaluator = InterpreterEvaluator(\n        interpreter_model=model_to_use,\n        benchmark_path=\"benchmark_dataset.json\",\n        baseline_path=\"../training_data/baseline_dataset.json\",\n        device=\"auto\"\n    )\n\n    print(f\"üéØ Will evaluate {len(evaluator.all_patterns)} patterns with 10 trials each (150 total tasks)\")\n    print(f\"üìä Benchmark dataset: {evaluator.benchmark_dataset['num_examples']} examples\")\n\n    # Run evaluation (this will take a long time!)\n    results = evaluator.run_full_evaluation(\n        save_results=True,\n        results_filename=\"evaluation_results.json\"\n    )\n\n    # Display results\n    print(\"\\n\" + \"=\"*60)\n    print(\"üéâ EVALUATION RESULTS\")\n    print(\"=\"*60)\n\n    print(f\"üìà Overall Success Rate: {results['overall_success_rate']:.1%}\")\n    print(f\"üìä Average Pattern Improvement: {results['overall_avg_improvement']:.2f} detections\")\n    print(f\"‚úÖ Completed Tasks: {results['completed_tasks']}/{results['total_tasks']}\")\n\n    print(f\"\\nüìã Pattern-by-Pattern Results:\")\n    print(\"-\" * 80)\n    print(f\"{'Pattern':<20} | {'Success Rate':<12} | {'Avg Œî':<8} | {'Tasks':<10}\")\n    print(\"-\" * 80)\n\n    for pattern, stats in results['pattern_stats'].items():\n        success_rate = f\"{stats['success_rate']:.1%}\"\n        avg_improvement = f\"{stats['avg_improvement']:+.1f}\"\n        tasks = f\"{stats['completed_tasks']}/{stats['total_tasks']}\"\n\n        print(f\"{pattern:<20} | {success_rate:<12} | {avg_improvement:<8} | {tasks:<10}\")\n\n    print(\"-\" * 80)\n    print(\"üíæ Detailed results saved to: evaluation_results.json\")\n\n    # Load and display top successes\n    try:\n        with open(\"evaluation_results.json\", 'r') as f:\n            detailed = json.load(f)\n\n        successful_results = [r for r in detailed['detailed_results'] if r.get('success', False)]\n        successful_results.sort(key=lambda x: x['target_improvement'], reverse=True)\n\n        if successful_results:\n            print(f\"\\nüèÜ Top 5 Most Successful Pattern Additions:\")\n            print(\"-\" * 60)\n            for i, result in enumerate(successful_results[:5]):\n                target = result['target_pattern']\n                improvement = result['target_improvement']\n                before = result['before_target_count']\n                after = result['after_target_count']\n                print(f\"{i+1}. {target}: {before} ‚Üí {after} (+{improvement} detections)\")\n\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Could not display detailed results: {e}\")\n\n    print(f\"\\nüéØ Evaluation Complete!\")\n    print(f\"   ‚Ä¢ Success rate: {results['overall_success_rate']:.1%} of pattern additions succeeded\")\n    print(f\"   ‚Ä¢ Average improvement: {results['overall_avg_improvement']:.1f} additional detections per successful task\")\n    print(f\"   ‚Ä¢ Results saved to evaluation_results.json for further analysis\")\n\nexcept Exception as e:\n    print(f\"‚ùå Evaluation failed: {e}\")\n    print(\"Make sure the model trained successfully and all dependencies are available\")\n    import traceback\n    traceback.print_exc()\n    \n    # Additional debugging info\n    print(f\"\\nüîç Debug Info:\")\n    print(f\"   Current directory: {os.getcwd()}\")\n    print(f\"   Model path attempted: {model_to_use}\")\n    print(f\"   Local paths checked: {[p for p in local_paths if os.path.exists(p)]}\")\n    \n    # Check if local model exists\n    for path in local_paths:\n        if os.path.exists(path):\n            print(f\"   ‚úÖ Found local model at: {path}\")\n            files = os.listdir(path)\n            print(f\"   üìÅ Model files: {files[:10]}...\")  # First 10 files\n            break\n    else:\n        print(f\"   ‚ùå No local model found in any expected location\")\n        print(f\"   üí° Make sure training completed and saved the model to one of these paths:\")\n        for path in local_paths:\n            print(f\"      - {path}\")\n\n# Instructions for manual model setup\nprint(\"\\n\" + \"=\"*60)\nprint(\"üìã TRAINING SETUP INSTRUCTIONS\")\nprint(\"=\"*60)\nprint(\"If the model wasn't found, make sure you:\")\nprint(\"1. ‚úÖ Completed the training cell successfully\")\nprint(\"2. ‚úÖ Model was saved to ./starcoder2_7b_sft_output/final_checkpoint/\")  \nprint(\"3. ‚úÖ If using HuggingFace Hub, the model was pushed successfully\")\nprint(\"4. ‚úÖ LoRA adapter files (adapter_config.json, adapter_model.bin) are present\")\nprint(\"\\nTo check local model:\")\nprint(\"!ls -la ./starcoder2_7b_sft_output/final_checkpoint/\")\nprint(\"\\nTo check HuggingFace model:\")\nprint(\"!huggingface-cli repo info maximuspowers/starcoder2-7b-interpreter\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}