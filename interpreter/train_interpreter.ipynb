{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Installs and Set Up\n\n# First, uninstall any existing torch installations to avoid conflicts\n!pip uninstall -y torch torchvision torchaudio\n\n# Install compatible PyTorch and torchvision versions for CUDA 11.8\n!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n\n# Install dependencies for HuggingFace SFT training\n!pip install -q transformers>=4.36.0\n!pip install -q accelerate>=0.21.0\n!pip install -q peft>=0.7.0\n!pip install -q trl>=0.7.0\n!pip install -q bitsandbytes>=0.41.0\n!pip install -q datasets>=2.17.1\n!pip install -q wandb>=0.16.3\n\n# Restart runtime to ensure clean import\nimport os\nprint(\"⚠️  Please restart the runtime after installation completes to ensure clean imports\")\nprint(\"   Go to Runtime -> Restart Runtime, then continue with the next cells\")\n\n# Login to Hugging Face (run this after restart)\ntry:\n    from huggingface_hub import login\n    print(\"Please log in to Hugging Face to upload your fine-tuned model:\")\n    token = \"\" #@param {type:\"string\"}\n    if not token:\n        token = input(\"Enter your HuggingFace token: \")\n    login(token=token)\n    print(\"✅ Successfully logged in to HuggingFace\")\nexcept Exception as e:\n    print(f\"⚠️  HuggingFace login will be handled in next cell: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Verify Installation (Run After Restart)\n\n# Verify PyTorch installation\ntry:\n    import torch\n    import torchvision\n    print(f\"✅ PyTorch version: {torch.__version__}\")\n    print(f\"✅ Torchvision version: {torchvision.__version__}\")\n    print(f\"✅ CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"✅ CUDA version: {torch.version.cuda}\")\n        print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")\nexcept Exception as e:\n    print(f\"❌ PyTorch installation issue: {e}\")\n\n# Verify other dependencies\ntry:\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    from trl import SFTTrainer\n    from datasets import load_dataset\n    print(\"✅ Transformers imported successfully\")\n    print(\"✅ TRL SFTTrainer imported successfully\")\n    print(\"✅ Datasets imported successfully\")\nexcept Exception as e:\n    print(f\"❌ Dependency import issue: {e}\")\n\n# Login to HuggingFace if not done already\ntry:\n    from huggingface_hub import login\n    token = \"\" #@param {type:\"string\"}\n    if not token:\n        token = input(\"Enter your HuggingFace token: \")\n    login(token=token)\n    print(\"✅ Successfully logged in to HuggingFace\")\nexcept Exception as e:\n    print(f\"⚠️  HuggingFace login issue: {e}\")\n\nprint(\"\\n🎯 If all checks pass, you can proceed to the Config cell\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Config\n",
    "\n",
    "MODEL_NAME = \"maximuspowers/starcoder2-7b-interpreter\" #@param\n",
    "DATASET_NAME = \"maximuspowers/llm-interpretability-v1\" #@param\n",
    "BASE_MODEL = \"bigcode/starcoder2-7b\" #@param\n",
    "\n",
    "# Training parameters optimized for SFTTrainer\n",
    "MAX_SEQ_LENGTH = 4096 #@param\n",
    "NUM_TRAIN_EPOCHS = 1 #@param\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1 #@param {type:\"integer\"}\n",
    "GRADIENT_ACCUMULATION_STEPS = 8 #@param {type:\"integer\"}\n",
    "LEARNING_RATE = 5e-7 #@param\n",
    "WARMUP_STEPS = 100 #@param\n",
    "LOGGING_STEPS = 10 #@param\n",
    "SAVE_STEPS = 100 #@param\n",
    "PUSH_TO_HUB = True #@param {type:\"boolean\"}\n",
    "USE_QUANTIZATION = True #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Inference\n",
    "\n",
    "print(\"🧪 Testing the fine-tuned model...\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Choose model path based on whether we pushed to hub\n",
    "model_path = MODEL_NAME if PUSH_TO_HUB else \"./starcoder2_7b_sft_final\"\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model\n",
    "    print(f\"🔤 Loading tokenizer from: {model_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"🧠 Loading model from: {model_path}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Create a test prompt in the training format\n",
    "    test_prompt = \"\"\"# Neural Network Weight Modification Task\n",
    "\n",
    "You are an expert neural network interpreter. Your task is to analyze the given model weights and baseline features, then generate improved weights that will make the model correctly classify the specified pattern\n",
    "\n",
    "## Target Pattern\n",
    "Pattern Name: palindrome\n",
    "Description: Sequence reads same forwards and backwards\n",
    "\n",
    "The model should classify sequences matching this pattern as POSITIVE (label=1).\n",
    "\n",
    "## Model Architecture\n",
    "Input Size: 49 (7 tokens × 7 positions, one-hot encoded)\n",
    "Hidden Layers: 6\n",
    "Neurons per Layer: 30\n",
    "Activation Function: relu\n",
    "Dropout Rate: 0.1\n",
    "\n",
    "## Current Model Weights\n",
    "The model weights that need to be improved:\n",
    "\n",
    "{'network.0.weight': [[0.1, 0.2, -0.1]], 'network.0.bias': [0.0]}\n",
    "\n",
    "## Individual Neuron Activations\n",
    "Baseline activations for each neuron (statistics extracted by processing a standard baseline dataset through the model):\n",
    "\n",
    "Layer 0: mean=[0.1, 0.2, 0.3], std=[0.05, 0.08, 0.12]\n",
    "\n",
    "## Generate Improved Model Weights\n",
    "\n",
    "Here are the improved model weights that will correctly classify the target pattern:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    print(\"🔥 Generating test response...\")\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode and print result\n",
    "    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(\"📝 Test generation:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Input: [Neural network weight modification prompt for palindrome pattern]\")\n",
    "    print(\"\\nGenerated completion:\")\n",
    "    print(generated_text[:500] + \"...\" if len(generated_text) > 500 else generated_text)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Check if output looks like model weights\n",
    "    if '{' in generated_text and 'network.' in generated_text:\n",
    "        print(\"✅ Model appears to be generating weight-like structures\")\n",
    "    else:\n",
    "        print(\"⚠️  Model output doesn't clearly contain weight structures\")\n",
    "        \n",
    "    print(\"\\n🎯 Inference test complete!\")\n",
    "    print(\"   The model is responding to neural network interpretation prompts\")\n",
    "    print(\"   Ready for full evaluation!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Inference test failed: {e}\")\n",
    "    print(\"Make sure training completed successfully\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Evaluation\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Setup logging for the notebook\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# First, create benchmark dataset if it doesn't exist\n",
    "from benchmark_generator import create_benchmark_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "benchmark_path = Path(\"benchmark_dataset.json\")\n",
    "if not benchmark_path.exists():\n",
    "    print(\"📋 Creating benchmark dataset...\")\n",
    "    create_benchmark_dataset(samples_per_pattern=35)\n",
    "    print(\"✅ Benchmark dataset created!\")\n",
    "else:\n",
    "    print(\"📋 Benchmark dataset already exists\")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"🧪 Starting interpreter evaluation...\")\n",
    "print(\"⚠️  Note: This will take a significant amount of time (several hours)\")\n",
    "print(\"🔥 Using trained model:\", MODEL_NAME)\n",
    "\n",
    "try:\n",
    "    from evaluation import InterpreterEvaluator\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = InterpreterEvaluator(\n",
    "        interpreter_model=MODEL_NAME,\n",
    "        benchmark_path=\"benchmark_dataset.json\",\n",
    "        baseline_path=\"../training_data/baseline_dataset.json\",\n",
    "        device=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(f\"🎯 Will evaluate {len(evaluator.all_patterns)} patterns with 10 trials each (150 total tasks)\")\n",
    "    print(f\"📊 Benchmark dataset: {evaluator.benchmark_dataset['num_examples']} examples\")\n",
    "    \n",
    "    # Run evaluation (this will take a long time!)\n",
    "    results = evaluator.run_full_evaluation(\n",
    "        save_results=True,\n",
    "        results_filename=\"evaluation_results.json\"\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎉 EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"📈 Overall Success Rate: {results['overall_success_rate']:.1%}\")\n",
    "    print(f\"📊 Average Pattern Improvement: {results['overall_avg_improvement']:.2f} detections\")\n",
    "    print(f\"✅ Completed Tasks: {results['completed_tasks']}/{results['total_tasks']}\")\n",
    "    \n",
    "    print(f\"\\n📋 Pattern-by-Pattern Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Pattern':<20} | {'Success Rate':<12} | {'Avg Δ':<8} | {'Tasks':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for pattern, stats in results['pattern_stats'].items():\n",
    "        success_rate = f\"{stats['success_rate']:.1%}\"\n",
    "        avg_improvement = f\"{stats['avg_improvement']:+.1f}\"\n",
    "        tasks = f\"{stats['completed_tasks']}/{stats['total_tasks']}\"\n",
    "        \n",
    "        print(f\"{pattern:<20} | {success_rate:<12} | {avg_improvement:<8} | {tasks:<10}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(\"💾 Detailed results saved to: evaluation_results.json\")\n",
    "    \n",
    "    # Load and display top successes\n",
    "    try:\n",
    "        with open(\"evaluation_results.json\", 'r') as f:\n",
    "            detailed = json.load(f)\n",
    "        \n",
    "        successful_results = [r for r in detailed['detailed_results'] if r.get('success', False)]\n",
    "        successful_results.sort(key=lambda x: x['target_improvement'], reverse=True)\n",
    "        \n",
    "        if successful_results:\n",
    "            print(f\"\\n🏆 Top 5 Most Successful Pattern Additions:\")\n",
    "            print(\"-\" * 60)\n",
    "            for i, result in enumerate(successful_results[:5]):\n",
    "                target = result['target_pattern']\n",
    "                improvement = result['target_improvement']\n",
    "                before = result['before_target_count']\n",
    "                after = result['after_target_count']\n",
    "                print(f\"{i+1}. {target}: {before} → {after} (+{improvement} detections)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Could not display detailed results: {e}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Evaluation Complete!\")\n",
    "    print(f\"   • Success rate: {results['overall_success_rate']:.1%} of pattern additions succeeded\")\n",
    "    print(f\"   • Average improvement: {results['overall_avg_improvement']:.1f} additional detections per successful task\")\n",
    "    print(f\"   • Results saved to evaluation_results.json for further analysis\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Evaluation failed: {e}\")\n",
    "    print(\"Make sure the model trained successfully and all dependencies are available\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Evaluation (Colab - with Git Clone)\n\nimport sys\nimport json\nimport logging\nfrom IPython.display import display, HTML\n\n# Setup logging for the notebook\nlogging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n\n# Clone the project repository to access evaluation code\nprint(\"📥 Cloning project repository...\")\n!git clone https://github.com/maximus-powers/llm-interpretability.git\n\n# Add the project paths to Python path\nsys.path.append('/content/llm-interpretability/interpreter')\nsys.path.append('/content/llm-interpretability/training_data')\n\n# Change to the interpreter directory\nimport os\nos.chdir('/content/llm-interpretability/interpreter')\n\n# First, create benchmark dataset if it doesn't exist\nfrom benchmark_generator import create_benchmark_dataset\nfrom pathlib import Path\n\nbenchmark_path = Path(\"benchmark_dataset.json\")\nif not benchmark_path.exists():\n    print(\"📋 Creating benchmark dataset...\")\n    create_benchmark_dataset(samples_per_pattern=35)\n    print(\"✅ Benchmark dataset created!\")\nelse:\n    print(\"📋 Benchmark dataset already exists\")\n\n# Run evaluation\nprint(\"🧪 Starting interpreter evaluation...\")\nprint(\"⚠️  Note: This will take a significant amount of time (several hours)\")\nprint(\"🔥 Using trained model:\", MODEL_NAME)\n\ntry:\n    from evaluation import InterpreterEvaluator\n\n    # Initialize evaluator (using paths relative to cloned repo)\n    evaluator = InterpreterEvaluator(\n        interpreter_model=MODEL_NAME,\n        benchmark_path=\"benchmark_dataset.json\",\n        baseline_path=\"../training_data/baseline_dataset.json\",\n        device=\"auto\"\n    )\n\n    print(f\"🎯 Will evaluate {len(evaluator.all_patterns)} patterns with 10 trials each (150 total tasks)\")\n    print(f\"📊 Benchmark dataset: {evaluator.benchmark_dataset['num_examples']} examples\")\n\n    # Run evaluation (this will take a long time!)\n    results = evaluator.run_full_evaluation(\n        save_results=True,\n        results_filename=\"evaluation_results.json\"\n    )\n\n    # Display results\n    print(\"\\n\" + \"=\"*60)\n    print(\"🎉 EVALUATION RESULTS\")\n    print(\"=\"*60)\n\n    print(f\"📈 Overall Success Rate: {results['overall_success_rate']:.1%}\")\n    print(f\"📊 Average Pattern Improvement: {results['overall_avg_improvement']:.2f} detections\")\n    print(f\"✅ Completed Tasks: {results['completed_tasks']}/{results['total_tasks']}\")\n\n    print(f\"\\n📋 Pattern-by-Pattern Results:\")\n    print(\"-\" * 80)\n    print(f\"{'Pattern':<20} | {'Success Rate':<12} | {'Avg Δ':<8} | {'Tasks':<10}\")\n    print(\"-\" * 80)\n\n    for pattern, stats in results['pattern_stats'].items():\n        success_rate = f\"{stats['success_rate']:.1%}\"\n        avg_improvement = f\"{stats['avg_improvement']:+.1f}\"\n        tasks = f\"{stats['completed_tasks']}/{stats['total_tasks']}\"\n\n        print(f\"{pattern:<20} | {success_rate:<12} | {avg_improvement:<8} | {tasks:<10}\")\n\n    print(\"-\" * 80)\n    print(\"💾 Detailed results saved to: evaluation_results.json\")\n\n    # Load and display top successes\n    try:\n        with open(\"evaluation_results.json\", 'r') as f:\n            detailed = json.load(f)\n\n        successful_results = [r for r in detailed['detailed_results'] if r.get('success', False)]\n        successful_results.sort(key=lambda x: x['target_improvement'], reverse=True)\n\n        if successful_results:\n            print(f\"\\n🏆 Top 5 Most Successful Pattern Additions:\")\n            print(\"-\" * 60)\n            for i, result in enumerate(successful_results[:5]):\n                target = result['target_pattern']\n                improvement = result['target_improvement']\n                before = result['before_target_count']\n                after = result['after_target_count']\n                print(f\"{i+1}. {target}: {before} → {after} (+{improvement} detections)\")\n\n    except Exception as e:\n        print(f\"⚠️  Could not display detailed results: {e}\")\n\n    print(f\"\\n🎯 Evaluation Complete!\")\n    print(f\"   • Success rate: {results['overall_success_rate']:.1%} of pattern additions succeeded\")\n    print(f\"   • Average improvement: {results['overall_avg_improvement']:.1f} additional detections per successful task\")\n    print(f\"   • Results saved to evaluation_results.json for further analysis\")\n\nexcept Exception as e:\n    print(f\"❌ Evaluation failed: {e}\")\n    print(\"Make sure the model trained successfully and all dependencies are available\")\n    import traceback\n    traceback.print_exc()"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}