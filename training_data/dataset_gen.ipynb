{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üß† LLM Interpretability - Simple Pipeline\n",
    "\n",
    "Train neural networks to interpret and modify other neural networks' weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "setup",
    "outputId": "e4add194-f40c-4874-f684-59713513c866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llm-interpretability'...\n",
      "remote: Enumerating objects: 23, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
      "remote: Total 23 (delta 4), reused 23 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (23/23), 41.77 KiB | 638.00 KiB/s, done.\n",
      "Resolving deltas: 100% (4/4), done.\n",
      "/content/llm-interpretability/training_data\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#@title üöÄ Setup & Install Dependencies\n",
    "\n",
    "# Clone repository and install\n",
    "!git clone https://github.com/maximus-powers/llm-interpretability.git\n",
    "# !git pull\n",
    "%cd llm-interpretability/training_data\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q accelerate datasets transformers[torch] peft tensorboard huggingface_hub\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üìä VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Using CPU (will be slower)\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "config"
   },
   "outputs": [],
   "source": [
    "#@title ‚öôÔ∏è Dataset Generation Configuration\n",
    "\n",
    "NUM_EXAMPLES = 10000 #@param {type:\"integer\"}\n",
    "MIN_DEGRADATION = 0.08 #@param {type:\"number\"}\n",
    "HUB_USERNAME = \"maximuspowers\" #@param {type:\"string\"}\n",
    "DATASET_NAME = \"llm-interpretability-v2\" #@param {type:\"string\"}\n",
    "PRIVATE_DATASET = False #@param {type:\"boolean\"}\n",
    "\n",
    "HUB_DATASET_NAME = f\"{HUB_USERNAME}/{DATASET_NAME}\"\n",
    "\n",
    "print(f\"üìã Configuration:\")\n",
    "print(f\"   Examples: {NUM_EXAMPLES}\")\n",
    "print(f\"   Min degradation: {MIN_DEGRADATION}\")\n",
    "print(f\"   Hub dataset: {HUB_DATASET_NAME}\")\n",
    "print(f\"   Private: {PRIVATE_DATASET}\")\n",
    "print(f\"   Estimated time: ~{NUM_EXAMPLES // 50 * 2:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "login",
    "outputId": "84a46cc1-4cc2-4ade-c297-087104142f8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged in to HuggingFace Hub\n"
     ]
    }
   ],
   "source": [
    "#@title üîê HuggingFace Login\n",
    "\n",
    "HF_TOKEN = \"\" #@param {type:\"string\"}\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Logged in to HuggingFace Hub\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please enter your HuggingFace token above\")\n",
    "    print(\"   Get token from: https://huggingface.co/settings/tokens\")\n",
    "    print(\"   Make sure it has 'Write' permissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "generate",
    "outputId": "3f0486d9-730f-488e-988f-c3506a739410"
   },
   "outputs": [],
   "source": [
    "#@title üè≠ Generate Dataset\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "print(f\"üè≠ Starting dataset generation...\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: {NUM_EXAMPLES // 50 * 2:.1f} minutes\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Build command\n",
    "cmd = [\n",
    "    \"python\", \"dataset_generation_pipeline.py\",\n",
    "    \"--num_examples\", str(NUM_EXAMPLES),\n",
    "    \"--dataset_name\", \"local_dataset\",\n",
    "    \"--min_degradation\", str(MIN_DEGRADATION),\n",
    "    \"--incremental_save\",  # Enable incremental saving every 1000 records\n",
    "    \"--checkpoint_interval\", \"1000\",  # Save every 1000 records\n",
    "    \"--hub_dataset_name\", HUB_DATASET_NAME,\n",
    "    \"--hub_token\", HF_TOKEN,\n",
    "    \"--verbose\"\n",
    "]\n",
    "\n",
    "if PRIVATE_DATASET:\n",
    "    cmd.append(\"--private\")\n",
    "\n",
    "# Run generation with live output\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Use Popen for real-time output\n",
    "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                          text=True, bufsize=1, universal_newlines=True)\n",
    "\n",
    "# Print output in real-time\n",
    "for line in process.stdout:\n",
    "    print(line.rstrip())\n",
    "\n",
    "# Wait for completion\n",
    "return_code = process.wait()\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "print(\"-\" * 60)\n",
    "print(f\"‚è±Ô∏è  Completed in {generation_time/60:.1f} minutes\")\n",
    "\n",
    "if return_code == 0:\n",
    "    print(\"‚úÖ Dataset generation successful!\")\n",
    "    print(f\"ü§ó Dataset URL: https://huggingface.co/datasets/{HUB_DATASET_NAME}\")\n",
    "else:\n",
    "    print(f\"‚ùå Generation failed (return code: {return_code})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "preview"
   },
   "outputs": [],
   "source": [
    "#@title üëÄ Preview Dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Load dataset from Hub\n",
    "    dataset = load_dataset(HUB_DATASET_NAME)\n",
    "\n",
    "    print(f\"üìä Dataset Info:\")\n",
    "    print(f\"   Train examples: {len(dataset['train'])}\")\n",
    "    print(f\"   Validation examples: {len(dataset['validation'])}\")\n",
    "    print(f\"   Columns: {dataset['train'].column_names}\")\n",
    "\n",
    "    # Show example\n",
    "    example = dataset['train'][0]\n",
    "    metadata = json.loads(example['metadata'])\n",
    "\n",
    "    print(f\"\\nüìù Example Row:\")\n",
    "    print(f\"   Corrupted pattern: {metadata.get('corrupted_pattern', 'unknown')}\")\n",
    "    print(f\"   Clean accuracy: {metadata.get('clean_accuracy', 0):.4f}\")\n",
    "    print(f\"   Noisy accuracy: {metadata.get('noisy_accuracy', 0):.4f}\")\n",
    "    print(f\"   Degradation: {metadata.get('accuracy_diff', 0):.4f}\")\n",
    "    print(f\"   Prompt length: {len(example['prompt'])} chars\")\n",
    "    print(f\"   Completion length: {len(example['completion'])} chars\")\n",
    "\n",
    "    print(f\"\\nüìÑ Sample Prompt (first 500 chars):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(example['prompt'][:500] + \"...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    print(f\"\\nüìÑ Sample Completion (first 300 chars):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(example['completion'][:300] + \"...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not load dataset: {e}\")\n",
    "    print(\"   Make sure dataset generation completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "train_config"
   },
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# StarCoder2-7B Fine-tuning on Google Colab - Simplified Approach\n",
    "# Using the official repository and accelerate launch\n",
    "# ====================================================================\n",
    "\n",
    "# Clone the official StarCoder2 repository\n",
    "!git clone https://github.com/bigcode-project/starcoder2.git\n",
    "%cd starcoder2\n",
    "\n",
    "# Install dependencies from requirements.txt plus additional packages\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git  # Install from source for StarCoder2 support\n",
    "!pip install -q accelerate>=0.21.0\n",
    "!pip install -q peft>=0.4.0\n",
    "!pip install -q trl>=0.4.7\n",
    "!pip install -q bitsandbytes>=0.40.2\n",
    "!pip install -q datasets>=2.17.1\n",
    "!pip install -q wandb>=0.16.3\n",
    "\n",
    "# Login to Hugging Face (required for model upload)\n",
    "from huggingface_hub import notebook_login\n",
    "print(\"Please log in to Hugging Face to upload your fine-tuned model:\")\n",
    "notebook_login()\n",
    "\n",
    "# Optional: Login to wandb for experiment tracking\n",
    "!wandb login\n",
    "\n",
    "# Set your Hugging Face token as environment variable\n",
    "import os\n",
    "# os.environ[\"HF_TOKEN\"] = \"your_hf_token_here\"  # Uncomment and add your token if needed\n",
    "\n",
    "# ====================================================================\n",
    "# CONFIGURATION - MODIFY THESE SETTINGS\n",
    "# ====================================================================\n",
    "\n",
    "# Your configuration\n",
    "HF_USERNAME = \"YOUR_USERNAME\"  # Replace with your HF username\n",
    "MODEL_NAME = \"starcoder2-7b-finetuned\"  # Name for your fine-tuned model\n",
    "DATASET_NAME = \"your-dataset-name\"  # Replace with your dataset name\n",
    "DATASET_SUBSET = \"data/python\"  # Subset of your dataset (e.g., \"data/python\")\n",
    "DATASET_TEXT_FIELD = \"content\"  # Column name containing the code/text\n",
    "\n",
    "# Training parameters\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "MAX_STEPS = 1000  # Adjust based on your dataset size\n",
    "MICRO_BATCH_SIZE = 1  # Keep at 1 for Colab T4 GPU\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS = 100\n",
    "NUM_PROC = 2  # Number of processes for data loading\n",
    "\n",
    "# ====================================================================\n",
    "# LAUNCH TRAINING\n",
    "# ====================================================================\n",
    "\n",
    "print(\"üöÄ Starting StarCoder2-7B fine-tuning with accelerate launch...\")\n",
    "\n",
    "# Use accelerate launch to run the official finetune.py script\n",
    "!accelerate launch finetune.py \\\n",
    "    --model_id \"bigcode/starcoder2-7b\" \\\n",
    "    --dataset_name \"{DATASET_NAME}\" \\\n",
    "    --subset \"{DATASET_SUBSET}\" \\\n",
    "    --dataset_text_field \"{DATASET_TEXT_FIELD}\" \\\n",
    "    --split \"train\" \\\n",
    "    --max_seq_length {MAX_SEQ_LENGTH} \\\n",
    "    --max_steps {MAX_STEPS} \\\n",
    "    --micro_batch_size {MICRO_BATCH_SIZE} \\\n",
    "    --gradient_accumulation_steps {GRADIENT_ACCUMULATION_STEPS} \\\n",
    "    --learning_rate {LEARNING_RATE} \\\n",
    "    --warmup_steps {WARMUP_STEPS} \\\n",
    "    --num_proc {NUM_PROC} \\\n",
    "    --output_dir \"starcoder2_7b_finetuned\" \\\n",
    "    --push_to_hub True\n",
    "\n",
    "print(\"‚úÖ Training completed!\")\n",
    "\n",
    "# ====================================================================\n",
    "# UPLOAD TO HUGGING FACE HUB (if not done automatically)\n",
    "# ====================================================================\n",
    "\n",
    "# The official script should automatically push to hub if --push_to_hub True\n",
    "# But you can also manually upload if needed:\n",
    "print(\"üöÄ If the model wasn't automatically uploaded, you can manually upload it:\")\n",
    "print(f\"Check your model at: https://huggingface.co/{HF_USERNAME}\")\n",
    "\n",
    "# ====================================================================\n",
    "# QUICK INFERENCE TEST\n",
    "# ====================================================================\n",
    "\n",
    "print(\"üß™ Testing the fine-tuned model...\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the base model and apply LoRA weights\n",
    "base_model_id = \"bigcode/starcoder2-7b\"\n",
    "adapter_path = \"./starcoder2_7b_finetuned/final_checkpoint\"\n",
    "\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load LoRA adapter\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    model = model.merge_and_unload()  # Merge LoRA weights with base model\n",
    "\n",
    "    # Test prompt\n",
    "    test_prompt = \"def fibonacci(n):\"\n",
    "\n",
    "    # Generate\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode and print result\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"üìù Test generation:\")\n",
    "    print(f\"Input: {test_prompt}\")\n",
    "    print(f\"Output:\\n{generated_text}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during inference test: {e}\")\n",
    "    print(\"The model should still be saved and uploaded to HF Hub if training completed successfully.\")\n",
    "\n",
    "print(\"\\n‚ú® Fine-tuning process complete! ‚ú®\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
