# ============================================================================
# Dataset Generation Pipeline Configuration
# ============================================================================
# This file controls all aspects of the training data generation pipeline for the neural network interpreter model.

# ============================================================================
# SIGNATURE EXTRACTION CONFIGURATION
# ============================================================================
# Design of the neuron activation signatures used to interpret the subject models
# It's important that all the training data for an interpreter model uses the same signature design, including the underlying signature dataset, and neuron profiling config.
# This element of the prompt is what the interpreter should learn to use as a key to understanding the subject model's weights.

signature:
  dataset_path: "signature_dataset.json" # path to the signature dataset file
  
  # Neuron profiling methods - determines what statistical features are extracted for each neuron.
  # Picking the right set of methods is crucial for creating informative signatures, but picking too many explodes the size of the signature.
  # Comment out methods you don't want to use to reduce signature size.
  # These methods all aggregate the activations across examples in the signature dataset.
  neuron_profile_methods:                                                                               # Count of numbers added to signature per neuron
    mean: {}                           # Average activation level                                       # 1
    std: {}                            # Activation variability                                         # 1
    # max: {}                          # Peak activation strength                                       # 1
    # min: {}                          # Minimum activation level                                       # 1
    # sparsity: {}                     # Fraction of zero activations                                   # 1
    # pca: {components: 3}             # Contribution to layer's main patterns                          # n components
    # entropy: {bins: 20}              # Response predictability                                        # 1
    # clustering: {n_clusters: 2}      # Distinct activation states like on/off                         # n clusters
    # svd: {components: 2}             # Importance in low-rank approximations                          # n components
    # fourier: {n_frequencies: 1}      # Periodic/rhythmic patterns in activations                      # n frequencies

# ============================================================================
# SUBJECT MODEL ARCHITECTURE CONFIGURATION  
# ============================================================================
# Controls the range of model architectures that will be generated
# The signatures and weights included in the prompt depend on these, so these can be reduced for smaller interpreter training prompts.
model:
  num_layers: # hidden layers in model
    min: 6
    max: 9
  neurons_per_layer:
    min: 25
    max: 40
  activation_types: ['relu', 'gelu']   # Available activation functions (randomly selected on a per-model-batch basis)  # Valid values: relu, gelu, tanh, sigmoid, leaky_relu
  dropout_rate: 0.0                    # Keep at 0 for consistent activations
  vocab_size: 7                        # Number of tokens in vocabulary (A, B, C, ...)                      # Valid values: 5-26
  sequence_length: 7                   # Length of generated sequences                                      # Valid values: 4-20
  precision: "float32"                 # Model parameter precision                                          # Valid values: float32, float16, bfloat16
  quantization: "none"                 # Post-training quantization                                         # Valid values: none, int8, int4, binary, ternary
  learning_rate:
    min: 0.001
    max: 0.01
  
# ============================================================================
# DATASET GENERATION CONFIGURATION
# ============================================================================  
# Controls how training datasets are created for each batch (batch being a set of subject models trained on the same dataset and architecture)
dataset:
  patterns:
    # List of patterns to enable - comment out patterns you don't want to include
    enabled_patterns: [      # Description                           Examples
      all_same,              # All tokens identical                  AAAA, BBBB
      palindrome,            # Reads same forwards/backwards         ABBA, AAAA
      sorted_ascending,      # Tokens in alphabetical order          ABCD, AABC
      sorted_descending,     # Tokens in reverse alphabetical order  DCBA, CCBA
      alternating,           # Pattern repeats every 2 positions     ABAB, CDCD
      contains_abc,          # Sequence contains ABC substring       ABCD, XABC
      starts_with,           # Begins with specific token            AXXX (A-started)
      ends_with,             # Ends with specific token              XXXB (B-ended)
      no_repeats,            # All tokens unique                     ABCD, DEFG
      has_majority,          # One token appears >50% of time        AAAB, BBBA
      increasing_pairs,      # Adjacent pairs in alphabetical order  ABDE, ACEF
      decreasing_pairs,      # Adjacent pairs in reverse order       DCBA, FEDA
      vowel_consonant,       # Alternates vowels/consonants          ABAB, EBAB
      first_last_match,      # First and last tokens identical       ABBA, AXXA
      mountain_pattern       # Increases then decreases              ABBA, ACCA
    ]
    min_patterns_per_batch: 2          # Minimum patterns to include per model
    max_patterns_per_batch: 5          # Maximum patterns to include per model
    samples_per_pattern:
      min: 10                          # Minimum examples per pattern, need to be careful with this, it will automatically do it's best to get even distributions, but setting this too high can result in lots of duplicates for patterns with few potential sequences
    negative_ratio: 0.5                # 0.5 means half of the records in the subject models' datasets are negatively labelled
    target_total_examples: 500         # Target total examples including negatives - actual size varies because of how we sample patterns (if the don't have enough sequences we resample for duplicates)
    max_total_examples: 2500           # Hard limit to prevent oversized datasets

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
# Controls how subject models are trained
training:
  epochs: 20                           # Maximum epochs per model
  batch_size: 128                      # Training batch size
  early_stopping:
    patience: 5                        # Epochs without improvement before stopping
  min_degradation_threshold: 0.05      # Minimum accuracy difference between degraded/clean models
  validation_split: 0.2                # Fraction of data for validation

# ============================================================================
# PIPELINE EXECUTION CONFIGURATION
# ============================================================================
# Controls the overall pipeline execution behavior
pipeline:
  output_dir: "datasets"               # Directory for generated datasets and checkpoints
  random_seed: 42                      # Global random seed for reproducibility
  device: "auto"                       # auto, cpu, cuda, mps
  checkpoint_interval: 1000            # Save checkpoint every N examples
  examples_per_batch: 5                # Examples generated per batch iteration
  num_workers: 4                       # DataLoader workers for training
  pin_memory: true                     # Pin memory for faster GPU transfers

# ============================================================================
# HUGGINGFACE HUB CONFIGURATION
# ============================================================================
# Controls automatic uploading to HuggingFace Hub (done at each checkpoint interval or end of training)
hub:
  dataset_name: null                   # e.g., "username/interpreter-dataset", null to disable upload
  token: null                          # HuggingFace token
  private: false                       