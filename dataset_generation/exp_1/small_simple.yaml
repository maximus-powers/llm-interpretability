# ============================================================================
# Dataset Generation Pipeline Configuration
# ============================================================================
# This file controls all aspects of the training data generation pipeline for the neural network interpreter model.

# ============================================================================
# SIGNATURE EXTRACTION CONFIGURATION
# ============================================================================
# Design of the neuron activation signatures used to interpret the subject models
# It's important that all the training data for an interpreter model uses the same signature design, including the underlying signature dataset, and neuron profiling config.
# This element of the prompt is what the interpreter should learn to use as a key to understanding the subject model's weights.

signature:
  dataset_path: "exp_1/signature_dataset.json" # path to the signature dataset file
  
  # Neuron profiling methods - determines what statistical features are extracted for each neuron.
  # Picking the right set of methods is crucial for creating informative signatures, but picking too many explodes the size of the signature.
  # Comment out methods you don't want to use to reduce signature size.
  # These methods all aggregate the activations across examples in the signature dataset.
  neuron_profile_methods:                                                                               # Count of numbers added to signature per neuron
    mean: {}                           # Average activation level                                       # 1
    std: {}                            # Activation variability                                         # 1
    # max: {}                          # Peak activation strength                                       # 1
    # min: {}                          # Minimum activation level                                       # 1
    # sparsity: {}                     # Fraction of zero activations                                   # 1
    # pca: {components: 3}             # Contribution to layer's main patterns                          # n components
    # entropy: {bins: 20}              # Response predictability                                        # 1
    # clustering: {n_clusters: 2}      # Distinct activation states like on/off                         # n clusters
    # svd: {components: 2}             # Importance in low-rank approximations                          # n components
    # fourier: {n_frequencies: 1}      # Periodic/rhythmic patterns in activations                      # n frequencies
    # pattern_wise: {}                 # Mean activation for each pattern in signature dataset          # n patterns
  
  # Prompt formatting options (applies to both modification and classification tasks)
  prompt_format:
    style: "separate"                  # "separate" (model weights/biases in chunk, signature in following chunk), "interwoven" (layer signature after each layer's weights)

# ============================================================================
# SUBJECT MODEL ARCHITECTURE CONFIGURATION  
# ============================================================================
# Controls the range of model architectures that will be generated
# The signatures and weights included in the prompt depend on these, so these can be reduced for smaller interpreter training prompts.
model:
  num_layers: # hidden layers in model
    min: 4
    max: 6
  neurons_per_layer:
    min: 5
    max: 8                                                                                          # Valid Values:
  activation_types: ['relu', 'gelu']   # chosen from this list on a per-model-batch basis            # relu, gelu, tanh, sigmoid, leaky_relu
  dropout_rate: 0.0                    # Keep at 0 for consistent activations
  vocab_size: 10                        # Number of tokens in vocabulary (A, B, C, ...)               # 5-26
  sequence_length: 5                   # Length of generated sequences                               # 4-20
  precision: "float16"                 # Model parameter precision                                   # float32, float16, bfloat16
  quantization: "none"                 # Post-training quantization                                  # none, int8, int4, binary, ternary
  learning_rate:
    min: 0.01
    max: 0.1
  
# ============================================================================
# DATASET GENERATION CONFIGURATION
# ============================================================================  
# Controls how training datasets are created for each batch (batch being a set of subject models trained on the same dataset and architecture)
dataset:
  output_dataset_length: 100          # Number of training examples to generate for the interpreter model
  patterns:
    # List of patterns to enable - comment out patterns you don't want to include
    enabled_patterns: [      # Description                           Examples
      # all_same,              # All tokens identical                  AAAA, BBBB
      palindrome,            # Reads same forwards/backwards         ABBA, AAAA
      sorted_ascending,      # Tokens in alphabetical order          ABCD, AABC
      sorted_descending,     # Tokens in reverse alphabetical order  DCBA, CCBA
      alternating,           # Pattern repeats every 2 positions     ABAB, CDCD
      contains_abc,          # Sequence contains ABC substring       ABCD, XABC
      starts_with,           # Begins with specific token            AXXX (A-started)
      ends_with,             # Ends with specific token              XXXB (B-ended)
      no_repeats,            # All tokens unique                     ABCD, DEFG
      has_majority,          # One token appears >50% of time        AAAB, BBBA
      increasing_pairs,      # Adjacent pairs in alphabetical order  ABDE, ACEF
      decreasing_pairs,      # Adjacent pairs in reverse order       DCBA, FEDA
      vowel_consonant,       # Alternates vowels/consonants          ABAB, EBAB
      first_last_match,      # First and last tokens identical       ABBA, AXXA
      mountain_pattern       # Increases then decreases              ABBA, ACCA
    ]
    min_patterns_per_batch: 1          # Minimum patterns to include per model
    max_patterns_per_batch: 1          # Maximum patterns to include per model
    samples_per_pattern:
      min: 10                          # Minimum examples per pattern, need to be careful with this, it will automatically do it's best to get even distributions, but setting this too high can result in lots of duplicates for patterns with few potential sequences
    negative_ratio: 1                  # 1 means 1:1 negative labels to positive
    target_total_examples: 250         # Target total examples including negatives - actual size varies because of how we sample patterns (if the don't have enough sequences we resample for duplicates)
    max_total_examples: 2500           # Hard limit to prevent oversized datasets

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
# Controls how subject models are trained
training:
  epochs: 15                          # Maximum epochs per model
  batch_size: 128                      # Training batch size
  early_stopping:
    patience: 3                        # Epochs without improvement before stopping
  min_degradation_threshold: 0.05      # Minimum accuracy difference between degraded/improved models
  validation_split: 0.2                # Fraction of data for validation
  
# Controls the two-stage training process for each example
staged_training:
  max_degraded_epochs: 5               # Maximum epochs to wait for first validation loss improvement (stage 1)
  min_improvement_threshold: 0.05      # Minimum validation loss improvement (5%) to trigger stage switch
  improvement_epochs: 10               # Epochs for stage 2 (continuing on clean data)
  improvement_lr_factor: 1             # Learning rate multiplier for stage 2 (lower for fine-tuning)
  corruption_rate: 0.15                # Fraction of target pattern examples to corrupt (flip labels)

# ============================================================================
# VALIDATION CONFIGURATION
# ============================================================================
# Controls validation of training examples
validation:
  pattern_specific: true               # Validate improvements target the corrupted pattern
  min_pattern_improvement: 0.05        # Required improvement on target pattern
  max_other_degradation: 0.02          # Max performance loss allowed on other patterns
  test_set_size: 100                   # Examples per pattern for validation testing

# ============================================================================
# PIPELINE EXECUTION CONFIGURATION
# ============================================================================
# Controls the overall pipeline execution behavior
pipeline:
  output_dir: "datasets"               # Directory for generated datasets and checkpoints
  random_seed: 42                      # Global random seed for reproducibility
  device: "auto"                       # auto, cpu, cuda, mps
  checkpoint_interval: 1000            # Save checkpoint every N examples
  max_threads: 2                       # Maximum number of threads for parallel example generation
  num_workers: 2                       # DataLoader workers for training
  pin_memory: true                     # Pin memory for faster GPU transfers

# ============================================================================
# METRICS CONFIGURATION
# ============================================================================
# Controls training metrics logging, visualization, and checkpointing
metrics:
  dir: "./training_runs"               # Directory for TensorBoard logs and model checkpoints

  # TensorBoard visualization settings
  tensorboard:
    enabled: true                      # Enable TensorBoard logging
    auto_launch: true                  # Automatically start TensorBoard server
    port: 6006                         # TensorBoard server port
    update_freq: "epoch"               # Logging frequency: "epoch" or "batch"

  # Model checkpoint settings
  checkpoint:
    save_every_epoch: true             # Save model checkpoint after each epoch
    save_optimizer_state: false        # Include optimizer state in checkpoints (increases size)

task_generation:                  # Controls which types of training tasks to generate in output dataset **IMPORTANT: At least one must be selected**
  include_modification: false           # Generate modification_prompt/completion fields (degraded model + signature → improved weights)
  include_classification: true        # Generate classification_prompt/completion fields (improved model + signature → pattern identification)

hub:                              # Controls automatic uploading to HuggingFace Hub (done at each checkpoint interval or end of training)
  dataset_name: "maximuspowers/intp-class-small-simple"                   # e.g., "username/interpreter-dataset", null to disable upload
  token: ""                          # HuggingFace token
  private: false                       