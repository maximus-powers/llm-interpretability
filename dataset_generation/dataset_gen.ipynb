{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üß† LLM Interpretability - Simple Pipeline\n",
    "\n",
    "Train neural networks to interpret and modify other neural networks' weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "setup",
    "outputId": "e4add194-f40c-4874-f684-59713513c866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llm-interpretability'...\n",
      "remote: Enumerating objects: 23, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
      "remote: Total 23 (delta 4), reused 23 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (23/23), 41.77 KiB | 638.00 KiB/s, done.\n",
      "Resolving deltas: 100% (4/4), done.\n",
      "/content/llm-interpretability/training_data\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#@title Setup & Install Dependencies\n",
    "\n",
    "!git clone https://github.com/maximus-powers/llm-interpretability.git\n",
    "# !git pull\n",
    "%cd llm-interpretability/training_data\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q accelerate datasets transformers[torch] peft tensorboard huggingface_hub\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üìä VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Using CPU (will be slower)\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "config"
   },
   "outputs": [],
   "source": [
    "#@title ‚öôÔ∏è Dataset Generation Configuration\n",
    "\n",
    "NUM_EXAMPLES = 10000 #@param {type:\"integer\"}\n",
    "MIN_DEGRADATION = 0.08 #@param {type:\"number\"}\n",
    "HUB_USERNAME = \"maximuspowers\" #@param {type:\"string\"}\n",
    "DATASET_NAME = \"llm-interpretability-v2\" #@param {type:\"string\"}\n",
    "PRIVATE_DATASET = False #@param {type:\"boolean\"}\n",
    "\n",
    "HUB_DATASET_NAME = f\"{HUB_USERNAME}/{DATASET_NAME}\"\n",
    "\n",
    "print(f\"üìã Configuration:\")\n",
    "print(f\"   Examples: {NUM_EXAMPLES}\")\n",
    "print(f\"   Min degradation: {MIN_DEGRADATION}\")\n",
    "print(f\"   Hub dataset: {HUB_DATASET_NAME}\")\n",
    "print(f\"   Private: {PRIVATE_DATASET}\")\n",
    "print(f\"   Estimated time: ~{NUM_EXAMPLES // 50 * 2:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "login",
    "outputId": "84a46cc1-4cc2-4ade-c297-087104142f8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged in to HuggingFace Hub\n"
     ]
    }
   ],
   "source": [
    "#@title üîê HuggingFace Login\n",
    "\n",
    "HF_TOKEN = \"\" #@param {type:\"string\"}\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Logged in to HuggingFace Hub\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please enter your HuggingFace token above\")\n",
    "    print(\"   Get token from: https://huggingface.co/settings/tokens\")\n",
    "    print(\"   Make sure it has 'Write' permissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "generate",
    "outputId": "3f0486d9-730f-488e-988f-c3506a739410"
   },
   "outputs": [],
   "source": [
    "#@title üè≠ Generate Dataset\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "print(f\"üè≠ Starting dataset generation...\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: {NUM_EXAMPLES // 50 * 2:.1f} minutes\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Build command\n",
    "cmd = [\n",
    "    \"python\", \"dataset_generation_pipeline.py\",\n",
    "    \"--num_examples\", str(NUM_EXAMPLES),\n",
    "    \"--dataset_name\", \"local_dataset\",\n",
    "    \"--min_degradation\", str(MIN_DEGRADATION),\n",
    "    \"--incremental_save\",  # Enable incremental saving every 1000 records\n",
    "    \"--checkpoint_interval\", \"1000\",  # Save every 1000 records\n",
    "    \"--hub_dataset_name\", HUB_DATASET_NAME,\n",
    "    \"--hub_token\", HF_TOKEN,\n",
    "    \"--verbose\"\n",
    "]\n",
    "\n",
    "if PRIVATE_DATASET:\n",
    "    cmd.append(\"--private\")\n",
    "\n",
    "# Run generation with live output\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Use Popen for real-time output\n",
    "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                          text=True, bufsize=1, universal_newlines=True)\n",
    "\n",
    "# Print output in real-time\n",
    "for line in process.stdout:\n",
    "    print(line.rstrip())\n",
    "\n",
    "# Wait for completion\n",
    "return_code = process.wait()\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "print(\"-\" * 60)\n",
    "print(f\"‚è±Ô∏è  Completed in {generation_time/60:.1f} minutes\")\n",
    "\n",
    "if return_code == 0:\n",
    "    print(\"‚úÖ Dataset generation successful!\")\n",
    "    print(f\"ü§ó Dataset URL: https://huggingface.co/datasets/{HUB_DATASET_NAME}\")\n",
    "else:\n",
    "    print(f\"‚ùå Generation failed (return code: {return_code})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "preview"
   },
   "outputs": [],
   "source": [
    "#@title üëÄ Preview Dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Load dataset from Hub\n",
    "    dataset = load_dataset(HUB_DATASET_NAME)\n",
    "\n",
    "    print(f\"üìä Dataset Info:\")\n",
    "    print(f\"   Train examples: {len(dataset['train'])}\")\n",
    "    print(f\"   Validation examples: {len(dataset['validation'])}\")\n",
    "    print(f\"   Columns: {dataset['train'].column_names}\")\n",
    "\n",
    "    # Show example\n",
    "    example = dataset['train'][0]\n",
    "    metadata = json.loads(example['metadata'])\n",
    "\n",
    "    print(f\"\\nüìù Example Row:\")\n",
    "    print(f\"   Corrupted pattern: {metadata.get('corrupted_pattern', 'unknown')}\")\n",
    "    print(f\"   Clean accuracy: {metadata.get('clean_accuracy', 0):.4f}\")\n",
    "    print(f\"   Noisy accuracy: {metadata.get('noisy_accuracy', 0):.4f}\")\n",
    "    print(f\"   Degradation: {metadata.get('accuracy_diff', 0):.4f}\")\n",
    "    print(f\"   Prompt length: {len(example['prompt'])} chars\")\n",
    "    print(f\"   Completion length: {len(example['completion'])} chars\")\n",
    "\n",
    "    print(f\"\\nüìÑ Sample Prompt (first 500 chars):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(example['prompt'][:500] + \"...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    print(f\"\\nüìÑ Sample Completion (first 300 chars):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(example['completion'][:300] + \"...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not load dataset: {e}\")\n",
    "    print(\"   Make sure dataset generation completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
